{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as r\n",
    "import pandas as pd\n",
    "import re,os\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "198\n",
      "296\n",
      "396\n",
      "495\n",
      "595\n"
     ]
    }
   ],
   "source": [
    "import csv,json\n",
    "\n",
    "def deduplicate_list_of_set(l):\n",
    "    seen = set()\n",
    "    result = []\n",
    "    keys = list(l[0].keys())\n",
    "    for dic in l:\n",
    "        item = []\n",
    "        for key in keys:\n",
    "            item.append(dic[key])\n",
    "        item_tuple = tuple(item)\n",
    "        if item_tuple in seen:\n",
    "            continue\n",
    "        result.append(dic)\n",
    "        seen.add(item_tuple)\n",
    "    return result\n",
    "        \n",
    "\n",
    "def get_metadata_list(game_name)->list:\n",
    "    metadata_list = []\n",
    "    for i in range(0,6):\n",
    "        with open(f'video_list/list_{i}.json') as fin:\n",
    "            js = json.load(fin)\n",
    "            metadata_list += js['data']\n",
    "            print(len(metadata_list)) \n",
    "    metadata_list = deduplicate_list_of_set(metadata_list) \n",
    "    return metadata_list\n",
    "    \n",
    "metadata_list = get_metadata_list('minecraft')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_chat_log(metadata_list):\n",
    "    import os\n",
    "    valid_list = []\n",
    "    with open('video_list/urls.txt', 'w') as f:\n",
    "        for id, metadata in enumerate(metadata_list):\n",
    "            url = metadata['url']\n",
    "            f.write(url+'\\n')\n",
    "        # chat = ChatDownloader().get_chat(url) \n",
    "        # for message in chat:                        # iterate over messages\n",
    "        #     chat.print_formatted(message)\n",
    "        # with open(f\"{id}.json\", \"w\")as f:\n",
    "        #     f.write(json.dumps(chat, ensure_ascii=False))\n",
    "        \n",
    "\n",
    "handle_chat_log(metadata_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 590 urls.\n"
     ]
    }
   ],
   "source": [
    "with open('scrap_chat_log.sh', 'w')as f:\n",
    "    with open('video_list/urls.txt') as fin:\n",
    "        raw = fin.readlines()\n",
    "    print(f\"Total {len(raw)} urls.\")\n",
    "    for i, line in enumerate(raw):\n",
    "        f.write(f'chat_downloader {line.strip()} --output chat_log/{i}.json\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_urls():\n",
    "    l = []\n",
    "    for file in os.listdir('chat_log'):\n",
    "        l.append(int(file.split('.')[0]))\n",
    "    with open('video_list/urls.txt') as fin:\n",
    "        raw = fin.readlines()\n",
    "        print(f\"Before filtering, we have total {len(raw)} urls.\")\n",
    "    filtered_l = []\n",
    "    for i in l:\n",
    "        filtered_l.append(raw[i])\n",
    "    with open('video_list/urls.txt', 'w') as fout:\n",
    "        for line in filtered_l:\n",
    "            fout.write(line)\n",
    "    print(f\"Before filtering, we have total {len(filtered_l)} urls.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caohanwen/miniconda3/envs/venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/caohanwen/Desktop/TWICH/scrap.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/caohanwen/Desktop/TWICH/scrap.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspacy_langdetect\u001b[39;00m \u001b[39mimport\u001b[39;00m LanguageDetector\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/caohanwen/Desktop/TWICH/scrap.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/caohanwen/Desktop/TWICH/scrap.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_english\u001b[39m(sent):\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/site-packages/spacy_langdetect/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mspacy_langdetect\u001b[39;00m \u001b[39mimport\u001b[39;00m LanguageDetector\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/site-packages/spacy_langdetect/spacy_langdetect.py:3\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangdetect\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlang_detect_exception\u001b[39;00m \u001b[39mimport\u001b[39;00m LangDetectException\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangdetect\u001b[39;00m \u001b[39mimport\u001b[39;00m detect_langs\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspacy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokens\u001b[39;00m \u001b[39mimport\u001b[39;00m Doc\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_detect_language\u001b[39m(spacy_object):\n\u001b[1;32m      7\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/site-packages/spacy/__init__.py:14\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mthinc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m prefer_gpu, require_gpu, require_cpu  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mthinc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m Config\n\u001b[0;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcli\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minfo\u001b[39;00m \u001b[39mimport\u001b[39;00m info  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mglossary\u001b[39;00m \u001b[39mimport\u001b[39;00m explain  \u001b[39m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/site-packages/spacy/pipeline/__init__.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mentityruler\u001b[39;00m \u001b[39mimport\u001b[39;00m EntityRuler\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlemmatizer\u001b[39;00m \u001b[39mimport\u001b[39;00m Lemmatizer\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmorphologizer\u001b[39;00m \u001b[39mimport\u001b[39;00m Morphologizer\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpipe\u001b[39;00m \u001b[39mimport\u001b[39;00m Pipe\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtrainable_pipe\u001b[39;00m \u001b[39mimport\u001b[39;00m TrainablePipe\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:404\u001b[0m, in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from spacy_langdetect import LanguageDetector\n",
    "import spacy\n",
    "\n",
    "def is_english(sent):\n",
    "    nlp = spacy.load('en')\n",
    "    nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n",
    "    detected = doc = nlp(sent)._.language['language']\n",
    "    if detected == \"en\":\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "print(is_english(\"Hello world!\"))\n",
    "print(is_english(\"今天天气真厚\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 3 (445862045.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [13]\u001b[0;36m\u001b[0m\n\u001b[0;31m    def write_to_csv(csv_name, metadata_list):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 3\n"
     ]
    }
   ],
   "source": [
    "import json,csv\n",
    "from spacy_langdetect import LanguageDetector\n",
    "import spacy\n",
    "\n",
    "def is_english(sent):\n",
    "    nlp = spacy.load('en')\n",
    "    nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n",
    "    detected = doc = nlp(sent)._.language['language']\n",
    "    if detected == \"en\":\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def write_to_csv(csv_name, metadata_list):\n",
    "    from datetime import datetime,timezone,timedelta\n",
    "\n",
    "    def get_time(start_time):\n",
    "        start_time_obj = datetime.fromisoformat(start_time[:-1]).astimezone(timezone.utc)\n",
    "        return str(start_time_obj) \n",
    "\n",
    "    def add_time_eps(start_time, time_in_seconds:int):\n",
    "        start_time_obj = datetime.fromisoformat(start_time[:-1]).astimezone(timezone.utc)\n",
    "        delta = timedelta(seconds = time_in_seconds)\n",
    "        return str(delta + start_time_obj)\n",
    "\n",
    "    # gift price\n",
    "    price = {1: 4.99, 2: 9.99, 3: 24.99,}\n",
    "    \n",
    "    fieldnames = ['id', # identifier of the video \n",
    "        'stream_id',\n",
    "        'user_id',\n",
    "        'user_login',\n",
    "        'user_name',\n",
    "        'title',\n",
    "        'description',\n",
    "        'url',\n",
    "        'thumbnail_url',\n",
    "        'viewable',\n",
    "        'view_count',\n",
    "        'language',\n",
    "        'type',\n",
    "        'duration',\n",
    "        'video_created_at',\n",
    "        'comment_created_at',\n",
    "        'muted_segments',\n",
    "        'time_in_seconds',  # time of comment sent after video begins (in second)\n",
    "        'comment',  # comment message\n",
    "        'author_name', # comment user\n",
    "        'author_id', # identifier of author\n",
    "        'display_author_name', # The name of the author which is displayed to the viewer. This may be different to name.\n",
    "        'author_type',\n",
    "        'author_images',\n",
    "        'author_badges',\n",
    "        'author_subscribe_status',\n",
    "        'author_gender',\n",
    "        'author_is_banned',\n",
    "        'author_is_bot',\n",
    "        'author_is_non_coworker',\n",
    "        'author_is_original_poster',\n",
    "        'author_is_verified',\n",
    "        'message_type', # type of message\n",
    "        'emotes',\n",
    "        # gift states\n",
    "        'has_gift',\n",
    "        'gift_type',\n",
    "        'gift_tier',\n",
    "        'gift_amount',\n",
    "        'gift_sum',\n",
    "        'gift_receiver',\n",
    "        ]\n",
    "    with open(csv_name, 'w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames = fieldnames)   \n",
    "        writer.writeheader() \n",
    "        filename_list = os.listdir('chat_log')\n",
    "        for i, filename in enumerate(filename_list):\n",
    "            # record information about last community-gift-giving deed\n",
    "            last_sender = None\n",
    "            with open(f'chat_log/{filename}') as fin:\n",
    "                js = json.load(fin)\n",
    "            for item in js:\n",
    "                row = {'id': i, }\n",
    "                try:\n",
    "                    del metadata_list[i]['id']\n",
    "                except KeyError:\n",
    "                    pass\n",
    "                try:\n",
    "                    del metadata_list[i]['language']# all english!\n",
    "                except KeyError:\n",
    "                    pass\n",
    "                try:\n",
    "                    created_at = metadata_list[i]['created_at']\n",
    "                    del metadata_list[i]['created_at'] \n",
    "                    published_at = metadata_list[i]['published_ats']\n",
    "                    del metadata_list[i]['published_ats']\n",
    "                    assert(created_at == published_at)\n",
    "                    published_at_str = get_time(published_at) \n",
    "                except KeyError:\n",
    "                    pass\n",
    "                row['video_created_at'] = published_at_str\n",
    "                row.update(metadata_list[i])\n",
    "                try:\n",
    "                    row['time_in_seconds'] = item['time_in_seconds']\n",
    "                    row['comment_created_at'] = add_time_eps(published_at, item['time_in_seconds'])\n",
    "                except KeyError:\n",
    "                    pass\n",
    "                try:\n",
    "                    row['comment'] =  item['message']\n",
    "                    give_individual_sub = 'gifted a Tier' \n",
    "                    give_community_sub = 'is gifting'\n",
    "                    if give_community_sub in item['message']:\n",
    "                        last_sender = item['author']['id']\n",
    "                        msg = item['message'][(item['message'].find(give_community_sub) + len(give_community_sub)):] \n",
    "                        msg = msg.split()\n",
    "                        row['has_gift'] = True\n",
    "                        row['gift_tier'] = int(msg[2])\n",
    "                        row['gift_amount'] = int(msg[0])\n",
    "                        row['gift_receiver'] = msg[5] + \" \" + msg[6]\n",
    "                        row['gift_sum'] = row['gift_amount'] * price[row['gift_tier']]\n",
    "                        row['gift_type'] = \"To community\" \n",
    "                    elif give_individual_sub in item['message']:\n",
    "                        if last_sender != item['author']['id']:\n",
    "                            msg = item['message'][item['message'].find(give_individual_sub) + len(give_individual_sub):]\n",
    "                            msg = msg.split()\n",
    "                            row['has_gift'] = True\n",
    "                            row['gift_tier'] = int(msg[0])\n",
    "                            row['gift_amount'] = 1\n",
    "                            row['gift_receiver'] = msg[3]\n",
    "                            row['gift_sum'] = row['gift_amount'] * price[row['gift_tier']]\n",
    "                            row['gift_type'] = \"To specific user\"\n",
    "                        else:\n",
    "                            row['has_gift'] = False \n",
    "                    else:\n",
    "                        row['has_gift'] = False\n",
    "                except KeyError:\n",
    "                    pass\n",
    "                try:\n",
    "                    row['author_name'] = item['author']['name']\n",
    "                except KeyError:\n",
    "                    pass\n",
    "                try:\n",
    "                    row['author_id'] = item['author']['id']\n",
    "                except KeyError:\n",
    "                    pass\n",
    "                try:\n",
    "                    row['display_author_name'] = item['author']['display_name']\n",
    "                except KeyError:\n",
    "                    pass\n",
    "                try:\n",
    "                    row['author_type'] = item['author']['type']\n",
    "                except KeyError:\n",
    "                    pass\n",
    "                try:\n",
    "                    row['author_images'] = item['author']['images']\n",
    "                except KeyError:\n",
    "                    pass\n",
    "                try:\n",
    "                    row['author_badges'] = item['author']['badges']\n",
    "                    row['author_subscribe_status'] =  item['author']['badges']['description']\n",
    "                except KeyError:\n",
    "                    pass\n",
    "                try:\n",
    "                    row['author_gender'] = item['author']['gender']\n",
    "                except KeyError:\n",
    "                    pass\n",
    "                try:\n",
    "                    row['author_is_banned'] = item['author']['is_banned']\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    row['author_is_bot' ] = item['author']['is_bot']\n",
    "                except KeyError:\n",
    "                    pass\n",
    "                try:\n",
    "                    row['author_is_non_coworker'] =  item['author']['is_non_coworker']\n",
    "                except KeyError:\n",
    "                    pass\n",
    "                try:\n",
    "                    row['author_is_original_poster'] = item['author']['is_original_poster']\n",
    "                except KeyError:\n",
    "                    pass\n",
    "                try:\n",
    "                    row['author_is_verified'] =item['author']['is_verified']\n",
    "                except KeyError:\n",
    "                    pass\n",
    "                try:\n",
    "                    row['message_type'] = item['message_type']\n",
    "                except KeyError:\n",
    "                    pass\n",
    "                try:\n",
    "                    row['emotes'] = item['emotes']\n",
    "                except KeyError:\n",
    "                    pass\n",
    "                writer.writerow(row)\n",
    "            \n",
    "write_to_csv(\"clean/metadata.csv\", metadata_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymkv import MKVFile\n",
    "def extract_frames(mkv_file):\n",
    "     \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2e318355dd958e186acbe06194bcfca9ef1b2adc2beb37ffaf0e6c97265a294"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
